{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 标量、向量、矩阵和张量\n",
    "标量、向量、矩阵和张量是线性代数中的基本概念，它们在深度学习框架如TensorFlow和PyTorch中广泛使用。\n",
    "\n",
    "1. **标量（Scalar）**:\n",
    "   - 解释：标量是一个单独的数值，没有方向。在数学中，它通常表示为一个实数或复数。\n",
    "\n",
    "2. **向量（Vector）**:\n",
    "   - 解释：向量是一组有序的数值，可以表示空间中的点或方向。在数学中，它通常表示为一个一维数组。\n",
    "\n",
    "3. **矩阵（Matrix）**:\n",
    "   - 解释：矩阵是一个二维数组，可以表示线性变换、方程组的系数等。在数学中，它由行和列组成。\n",
    "\n",
    "4. **张量（Tensor）**:\n",
    "   - 解释：张量是一个多维数组，可以看作是标量、向量和矩阵的推广。在深度学习中，张量用于表示数据和参数，如图像、权重等。\n",
    "\n",
    "在TensorFlow和PyTorch中，张量是最基本的数据结构，用于表示各种维度的数据。这两个框架提供了丰富的操作来处理张量，如加法、乘法、转置等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow中，`tf.matmul` 和 `tf.multiply` 是两个不同的操作，它们分别用于执行矩阵乘法和逐元素乘法。\n",
    "\n",
    "1. **`tf.matmul`**:\n",
    "   - 用于执行两个矩阵的乘法，也称为点积或矩阵乘法。\n",
    "   - 结果矩阵的每个元素是第一个矩阵的行与第二个矩阵的列对应元素相乘后相加的结果。\n",
    "   - 用法示例：`c = tf.matmul(a, b)`，其中 `a` 和 `b` 是两个矩阵。\n",
    "\n",
    "2. **`tf.multiply`**:\n",
    "   - 用于执行两个张量的逐元素乘法，也称为哈达玛积或元素乘法。\n",
    "   - 结果张量的每个元素是输入张量对应位置元素的乘积。\n",
    "   - 用法示例：`c = tf.multiply(a, b)`，其中 `a` 和 `b` 是两个相同形状的张量。\n",
    "\n",
    "总结来说，`tf.matmul` 用于矩阵乘法，而 `tf.multiply` 用于逐元素乘法。两者在数学运算上有本质的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "b: tf.Tensor(\n",
      "[[2 4]\n",
      " [6 8]], shape=(2, 2), dtype=int32)\n",
      "c: tf.Tensor(\n",
      "[[14 20]\n",
      " [30 44]], shape=(2, 2), dtype=int32)\n",
      "d: tf.Tensor(\n",
      "[[ 2  8]\n",
      " [18 32]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "import tensorflow as tf\n",
    "\n",
    "# create a tensor\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "\n",
    "# tensor addition\n",
    "b = tf.add(a,a)\n",
    "\n",
    "# tensor multiplication\n",
    "c = tf.matmul(a,b)\n",
    "\n",
    "# tensor element-by-element multiplication\n",
    "d = tf.multiply(a,b)\n",
    "\n",
    "print(\"a:\",a)\n",
    "print(\"b:\",b)\n",
    "print(\"c:\",c)\n",
    "print(\"d:\",d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow中，`tf.constant` 和 `tf.Variable` 是创建张量的两种不同方式，它们有不同的用途和特性：\n",
    "\n",
    "1. **`tf.constant`**:\n",
    "   - 用于创建一个不可变的常量张量。一旦创建，这个张量的值就不能被改变。\n",
    "   - 常用于定义不需要在训练过程中改变的值，如模型的超参数、固定的数据等。\n",
    "   - 示例用法：\n",
    "     ```python\n",
    "     import tensorflow as tf\n",
    "\n",
    "     # 创建一个常量张量\n",
    "     a = tf.constant([[1, 2], [3, 4]])\n",
    "     print(a)\n",
    "     ```\n",
    "\n",
    "2. **`tf.Variable`**:\n",
    "   - 用于创建一个可变的变量张量。这个张量的值可以在训练过程中被更新和改变。\n",
    "   - 常用于定义模型的参数，如权重和偏置，这些参数需要在训练过程中通过反向传播算法进行更新。\n",
    "   - 示例用法：\n",
    "     ```python\n",
    "     # 创建一个变量张量\n",
    "     b = tf.Variable([[5, 6], [7, 8]])\n",
    "     print(b)\n",
    "\n",
    "     # 更新变量的值\n",
    "     b.assign([[9, 10], [11, 12]])\n",
    "     print(b)\n",
    "     ```\n",
    "\n",
    "总结来说，`tf.constant` 用于创建不可变的常量张量，而 `tf.Variable` 用于创建可变的变量张量，后者通常用于定义和更新模型的参数。在实际的深度学习模型中，两者都是非常重要和常用的。\n",
    "\n",
    "网址学习：https://blog.csdn.net/Forrest97/article/details/105913952?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170882916816800211534826%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=170882916816800211534826&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-105913952-null-null.142^v99^pc_search_result_base5&utm_term=with%20tf.GradientTape%28%29%20as%20tape%3A&spm=1018.2226.3001.4187\n",
    "\n",
    "在TensorFlow中，`tape.watch()` 是一个用于显式监视张量的方法。当你使用 `tf.GradientTape()` 时，默认情况下，它会自动监视所有的 `tf.Variable` 类型的张量，并记录它们的操作以便计算梯度。但是，对于非 `tf.Variable` 类型的张量（如使用 `tf.constant` 创建的张量），`GradientTape` 不会自动监视它们。\n",
    "\n",
    "如果你想要计算非 `tf.Variable` 类型的张量的梯度，你可以使用 `tape.watch()` 方法来显式地告诉 `GradientTape` 监视这个张量。这样，当你在 `GradientTape` 的上下文中对这个张量进行操作时，这些操作就会被记录下来，以便后续计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# example 2\n",
    "# Create a variable\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "# Use GradientTape to record the computation\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2\n",
    "\n",
    "# Compute the gradient of y with respect to x\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(\"dy/dx:\", dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dx:6.0\n",
      "dz_dx:108.0\n"
     ]
    }
   ],
   "source": [
    "# example3\n",
    "import tensorflow as tf\n",
    "\n",
    "x =tf.constant(3.0)\n",
    "\n",
    "# 当你将 persistent=True 设置给 GradientTape 时，它变成了持久的，\n",
    "# 这意味着你可以多次调用 tape.gradient() 方法来计算多个梯度，而不会导致资源的释放。\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2\n",
    "    z = y**2\n",
    "\n",
    "dy_dx = tape.gradient(y,x)\n",
    "dz_dx = tape.gradient(z,x)\n",
    "\n",
    "print(\"dy_dx:{}\\ndz_dx:{}\".format(dy_dx,dz_dx))\n",
    "#print(dz_dx) # 仅仅计算一个梯度的时候，无需persistent=True\n",
    "\n",
    "# 释放资源\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "b:tensor([[2, 4],\n",
      "        [6, 8]])\n",
      "c:tensor([[14, 20],\n",
      "        [30, 44]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([[1,2],[3,4]])\n",
    "\n",
    "b = a+a\n",
    "\n",
    "c = torch.matmul(a,b)\n",
    "\n",
    "print(\"a:{}\\nb:{}\\nc:{}\\n\".format(a,b,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# example 2\n",
    "# Using Autograd to Compute Gradients\n",
    "# create a tensor\n",
    "x =torch.tensor(3.0,requires_grad=True)\n",
    "# function\n",
    "y = x**2\n",
    "# Backpropagation to compute the gradient\n",
    "y.backward()\n",
    "\n",
    "print(\"dy/dx:\",x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(4.)\n",
      "dz/dy: tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "# example 3\n",
    "# 多元函数的梯度\n",
    "# requires_grad=True，这意味着PyTorch将会跟踪对 x 的操作以计算梯度。\n",
    "x = torch.tensor(2.0,requires_grad=True)\n",
    "y = torch.tensor(3.0,requires_grad=True)\n",
    "\n",
    "z = x**2+3*y**3\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"dz/dx:\",x.grad)\n",
    "print(\"dz/dy:\",y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor(29., grad_fn=<SumBackward0>)\n",
      "dy/dx: tensor([4., 6., 8.])\n"
     ]
    }
   ],
   "source": [
    "# example 4\n",
    "# 向量函数的梯度\n",
    "x =torch.tensor([1.0,2.0,3.0],requires_grad = True)\n",
    "y = torch.sum(x**2+2*x+1) # 结果是一个标量\n",
    "print(\"y:\",y)\n",
    "y.backward()\n",
    "print(\"dy/dx:\",x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dB/dA: tensor([[ 4., -3.],\n",
      "        [-2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# example 5\n",
    "# 矩阵函数的梯度\n",
    "import torch\n",
    "\n",
    "# 在PyTorch中，只有浮点数和复数类型的张量可以要求梯度.整数类型的张量 A 会导致运行时错误。上面tensorflow同样如此\n",
    "\n",
    "# A = torch.tensor([[1,2],[3,4]],requires_grad=True)\n",
    "A = torch.tensor([[1.0,2.0],[3.0,4.0]],requires_grad=True)\n",
    "B =torch.det(A)\n",
    "\n",
    "B.backward()\n",
    "print(\"dB/dA:\",A.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
