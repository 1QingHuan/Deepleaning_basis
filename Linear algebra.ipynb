{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 标量、向量、矩阵和张量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标量、向量、矩阵和张量是线性代数中的基本概念，它们在深度学习框架如TensorFlow和PyTorch中广泛使用。\n",
    "\n",
    "1. **标量（Scalar）**:\n",
    "   - 解释：标量是一个单独的数值，没有方向。在数学中，它通常表示为一个实数或复数。\n",
    "\n",
    "2. **向量（Vector）**:\n",
    "   - 解释：向量是一组有序的数值，可以表示空间中的点或方向。在数学中，它通常表示为一个一维数组。\n",
    "\n",
    "3. **矩阵（Matrix）**:\n",
    "   - 解释：矩阵是一个二维数组，可以表示线性变换、方程组的系数等。在数学中，它由行和列组成。\n",
    "\n",
    "4. **张量（Tensor）**:\n",
    "   - 解释：张量是一个多维数组，可以看作是标量、向量和矩阵的推广。在深度学习中，张量用于表示数据和参数，如图像、权重等。\n",
    "\n",
    "在TensorFlow和PyTorch中，张量是最基本的数据结构，用于表示各种维度的数据。这两个框架提供了丰富的操作来处理张量，如加法、乘法、转置等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow中，`tf.matmul` 和 `tf.multiply` 是两个不同的操作，它们分别用于执行矩阵乘法和逐元素乘法。\n",
    "\n",
    "1. **`tf.matmul`**:\n",
    "   - 用于执行两个矩阵的乘法，也称为点积或矩阵乘法。\n",
    "   - 结果矩阵的每个元素是第一个矩阵的行与第二个矩阵的列对应元素相乘后相加的结果。\n",
    "   - 用法示例：`c = tf.matmul(a, b)`，其中 `a` 和 `b` 是两个矩阵。\n",
    "\n",
    "2. **`tf.multiply`**:\n",
    "   - 用于执行两个张量的逐元素乘法，也称为哈达玛积或元素乘法。\n",
    "   - 结果张量的每个元素是输入张量对应位置元素的乘积。\n",
    "   - 用法示例：`c = tf.multiply(a, b)`，其中 `a` 和 `b` 是两个相同形状的张量。\n",
    "\n",
    "总结来说，`tf.matmul` 用于矩阵乘法，而 `tf.multiply` 用于逐元素乘法。两者在数学运算上有本质的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "b: tf.Tensor(\n",
      "[[2 4]\n",
      " [6 8]], shape=(2, 2), dtype=int32)\n",
      "c: tf.Tensor(\n",
      "[[14 20]\n",
      " [30 44]], shape=(2, 2), dtype=int32)\n",
      "d: tf.Tensor(\n",
      "[[ 2  8]\n",
      " [18 32]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "import tensorflow as tf\n",
    "\n",
    "# create a tensor\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "\n",
    "# tensor addition\n",
    "b = tf.add(a,a)\n",
    "\n",
    "# tensor multiplication\n",
    "c = tf.matmul(a,b)\n",
    "\n",
    "# tensor element-by-element multiplication\n",
    "d = tf.multiply(a,b)\n",
    "\n",
    "print(\"a:\",a)\n",
    "print(\"b:\",b)\n",
    "print(\"c:\",c)\n",
    "print(\"d:\",d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow中，`tf.constant` 和 `tf.Variable` 是创建张量的两种不同方式，它们有不同的用途和特性：\n",
    "\n",
    "1. **`tf.constant`**:\n",
    "   - 用于创建一个不可变的常量张量。一旦创建，这个张量的值就不能被改变。\n",
    "   - 常用于定义不需要在训练过程中改变的值，如模型的超参数、固定的数据等。\n",
    "   - 示例用法：\n",
    "     ```python\n",
    "     import tensorflow as tf\n",
    "\n",
    "     # 创建一个常量张量\n",
    "     a = tf.constant([[1, 2], [3, 4]])\n",
    "     print(a)\n",
    "     ```\n",
    "\n",
    "2. **`tf.Variable`**:\n",
    "   - 用于创建一个可变的变量张量。这个张量的值可以在训练过程中被更新和改变。\n",
    "   - 常用于定义模型的参数，如权重和偏置，这些参数需要在训练过程中通过反向传播算法进行更新。\n",
    "   - 示例用法：\n",
    "     ```python\n",
    "     # 创建一个变量张量\n",
    "     b = tf.Variable([[5, 6], [7, 8]])\n",
    "     print(b)\n",
    "\n",
    "     # 更新变量的值\n",
    "     b.assign([[9, 10], [11, 12]])\n",
    "     print(b)\n",
    "     ```\n",
    "\n",
    "总结来说，`tf.constant` 用于创建不可变的常量张量，而 `tf.Variable` 用于创建可变的变量张量，后者通常用于定义和更新模型的参数。在实际的深度学习模型中，两者都是非常重要和常用的。\n",
    "\n",
    "网址学习：https://blog.csdn.net/Forrest97/article/details/105913952?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170882916816800211534826%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=170882916816800211534826&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-105913952-null-null.142^v99^pc_search_result_base5&utm_term=with%20tf.GradientTape%28%29%20as%20tape%3A&spm=1018.2226.3001.4187\n",
    "\n",
    "在TensorFlow中，`tape.watch()` 是一个用于显式监视张量的方法。当你使用 `tf.GradientTape()` 时，默认情况下，它会自动监视所有的 `tf.Variable` 类型的张量，并记录它们的操作以便计算梯度。但是，对于非 `tf.Variable` 类型的张量（如使用 `tf.constant` 创建的张量），`GradientTape` 不会自动监视它们。\n",
    "\n",
    "如果你想要计算非 `tf.Variable` 类型的张量的梯度，你可以使用 `tape.watch()` 方法来显式地告诉 `GradientTape` 监视这个张量。这样，当你在 `GradientTape` 的上下文中对这个张量进行操作时，这些操作就会被记录下来，以便后续计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# example 2\n",
    "# Create a variable\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "# Use GradientTape to record the computation\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2\n",
    "\n",
    "# Compute the gradient of y with respect to x\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(\"dy/dx:\", dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dx:6.0\n",
      "dz_dx:108.0\n"
     ]
    }
   ],
   "source": [
    "# example3\n",
    "import tensorflow as tf\n",
    "\n",
    "x =tf.constant(3.0)\n",
    "\n",
    "# 当你将 persistent=True 设置给 GradientTape 时，它变成了持久的，\n",
    "# 这意味着你可以多次调用 tape.gradient() 方法来计算多个梯度，而不会导致资源的释放。\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2\n",
    "    z = y**2\n",
    "\n",
    "dy_dx = tape.gradient(y,x)\n",
    "dz_dx = tape.gradient(z,x)\n",
    "\n",
    "print(\"dy_dx:{}\\ndz_dx:{}\".format(dy_dx,dz_dx))\n",
    "#print(dz_dx) # 仅仅计算一个梯度的时候，无需persistent=True\n",
    "\n",
    "# 释放资源\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "b:tensor([[2, 4],\n",
      "        [6, 8]])\n",
      "c:tensor([[14, 20],\n",
      "        [30, 44]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([[1,2],[3,4]])\n",
    "\n",
    "b = a+a\n",
    "\n",
    "c = torch.matmul(a,b)\n",
    "\n",
    "print(\"a:{}\\nb:{}\\nc:{}\\n\".format(a,b,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# example 2\n",
    "# Using Autograd to Compute Gradients\n",
    "# create a tensor\n",
    "x =torch.tensor(3.0,requires_grad=True)\n",
    "# function\n",
    "y = x**2\n",
    "# Backpropagation to compute the gradient\n",
    "y.backward()\n",
    "\n",
    "print(\"dy/dx:\",x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(4.)\n",
      "dz/dy: tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "# example 3\n",
    "# 多元函数的梯度\n",
    "# requires_grad=True，这意味着PyTorch将会跟踪对 x 的操作以计算梯度。\n",
    "x = torch.tensor(2.0,requires_grad=True)\n",
    "y = torch.tensor(3.0,requires_grad=True)\n",
    "\n",
    "z = x**2+3*y**3\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"dz/dx:\",x.grad)\n",
    "print(\"dz/dy:\",y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor(29., grad_fn=<SumBackward0>)\n",
      "dy/dx: tensor([4., 6., 8.])\n"
     ]
    }
   ],
   "source": [
    "# example 4\n",
    "# 向量函数的梯度\n",
    "x =torch.tensor([1.0,2.0,3.0],requires_grad = True)\n",
    "y = torch.sum(x**2+2*x+1) # 结果是一个标量\n",
    "print(\"y:\",y)\n",
    "y.backward()\n",
    "print(\"dy/dx:\",x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dB/dA: tensor([[ 4., -3.],\n",
      "        [-2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# example 5\n",
    "# 矩阵函数的梯度\n",
    "import torch\n",
    "\n",
    "# 在PyTorch中，只有浮点数和复数类型的张量可以要求梯度.整数类型的张量 A 会导致运行时错误。上面tensorflow同样如此\n",
    "\n",
    "# A = torch.tensor([[1,2],[3,4]],requires_grad=True)\n",
    "A = torch.tensor([[1.0,2.0],[3.0,4.0]],requires_grad=True)\n",
    "B =torch.det(A)\n",
    "\n",
    "B.backward()\n",
    "print(\"dB/dA:\",A.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2矩阵和向量相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow版本\n",
    "`tf.linalg.matvec` 是 `TensorFlow` 中的一个函数，用于计算矩阵与向量的乘积。`tf.linalg` 是 `TensorFlow` 中专门用于线性代数计算的模块，`matvec` 则是 `matrix-vector` 的缩写，表示矩阵（`matrix`）和向量（`vector`）的乘法。\n",
    "\n",
    "具体来说，如果你有一个矩阵 `A` 和一个向量 `x`，使用 `tf.linalg.matvec(A, x)` 将会返回矩阵 `A` 与向量 `x` 的乘积结果。这个操作在数学上等同于将矩阵 `A` 中的每一行与向量 `x` 进行点积（内积），得到的结果是一个新的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14. 32.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 基本矩阵乘向量\n",
    "import tensorflow as tf\n",
    "\n",
    "matrix = tf.constant([[1,2],[3,4]],dtype=tf.float32)\n",
    "vector = tf.constant([4,5],dtype=tf.float32)\n",
    "\n",
    "result = tf.linalg.matvec(matrix,vector)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 5. 11.]\n",
      " [39. 53.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 批量矩阵乘向量\n",
    "'''\n",
    "在这个TensorFlow代码示例中,batch_matrix 是一个形状为 [2, 2, 2] 的张量，可以理解为包含两个 2x2 矩阵的批量数据。\n",
    "batch_vector 是一个形状为 [2, 2] 的张量，可以视作包含两个长度为 2 的向量的批量数据。\n",
    "tf.linalg.matvec 函数执行的是批量矩阵-向量乘法操作，对每个矩阵-向量对进行相应的乘法计算。\n",
    "前一个矩阵乘[1,2]=5,乘[3,4]=11;同理，后面。\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_matrix = tf.constant([[[1,2],[3,4]],[[5,6],[7,8]]],dtype=tf.float32)\n",
    "batch_vector = tf.constant([[1,2],[3,4]],dtype=tf.float32)\n",
    "batch_result = tf.linalg.matvec(batch_matrix,batch_vector)\n",
    "\n",
    "print(batch_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘矩阵\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定义两个矩阵\n",
    "matrix_a = tf.constant([[1, 2], \n",
    "                        [3, 4]], dtype=tf.float32)\n",
    "matrix_b = tf.constant([[5, 6], \n",
    "                        [7, 8]], dtype=tf.float32)\n",
    "\n",
    "# 矩阵乘矩阵\n",
    "matrix_product = tf.matmul(matrix_a,matrix_b)\n",
    "\n",
    "print(matrix_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch版本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17., 39.])\n"
     ]
    }
   ],
   "source": [
    "# 基本矩阵乘向量\n",
    "import torch\n",
    "\n",
    "matrix = torch.tensor([[1,2],[3,4]],dtype = torch.float32)\n",
    "vector = torch.tensor([5,6],dtype = torch.float32)\n",
    "\n",
    "result = torch.mv(matrix,vector)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们有一个批量的矩阵和一个批量的向量，我们想要计算每个矩阵与对应向量的乘积。\n",
    "\n",
    "1. **向量转换为列向量**:\n",
    "   - 在线性代数中，一个矩阵乘以一个列向量是一个标准的操作。但是在编程中，我们经常处理的向量默认是一维的，没有区分行向量或列向量。因此，为了进行矩阵与向量的乘法，我们需要先将这个向量转换为列向量。在PyTorch中，我们可以通过`unsqueeze`函数来添加一个新的维度。如果原始向量的形状是`[n, m]`，那么`unsqueeze(-1)`会在最后添加一个维度，变成`[n, m, 1]`，这样每个`[m]`向量就变成了`[m, 1]`的列向量。\n",
    "\n",
    "2. **执行批量的矩阵乘法**:\n",
    "   - `torch.matmul`函数可以处理高维度的张量乘法。当我们对批量的矩阵`[n, m, m]`和批量的列向量`[n, m, 1]`使用`torch.matmul`时，它会逐个矩阵地执行乘法。在这个例子中，`n`代表批量的大小，`m`代表矩阵的行和列的数量。`torch.matmul`会自动处理每个批次的矩阵和向量之间的乘积。\n",
    "\n",
    "3. **移除多余的维度**:\n",
    "   - 执行完乘法后，结果是一个形状为`[n, m, 1]`的张量，每个`[m, 1]`代表了一个矩阵-向量乘积的结果。因为最后的维度是1，它实际上是多余的，因为它不提供任何额外的信息（只有一列）。所以，我们使用`squeeze`函数来移除这个维度，使得最终的输出是`[n, m]`。现在，每行代表了一个矩阵与向量乘积的结果。\n",
    "\n",
    "在代码中，`batch_result = torch.matmul(batch_matrix, batch_vector.unsqueeze(-1)).squeeze(-1)`，这行代码简洁地将以上三步合并在一起，完成了从批量矩阵和批量向量到批量乘积结果的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在三维张量的矩阵乘法中，特定维度的大小必须相匹配，以满足乘法规则。对于两个张量进行矩阵乘法，我们通常关注以下维度：\n",
    "\n",
    "1. 最内侧的维度（列维度）：对于矩阵乘法来说，左侧矩阵的列数必须与右侧矩阵的行数相同。这是传统的两维矩阵乘法的要求。\n",
    "\n",
    "2. 批量维度：对于批量操作，通常批量的大小需要一致。即，如果我们有两个三维张量，它们的批量维度（通常是第一个维度）的大小需要匹配。这样才能保证每个批次的矩阵乘法都是对应进行的。\n",
    "\n",
    "以形状为 `[batch_size, n_rows, n_cols]` 的张量 A 和形状为 `[batch_size, n_cols, p]` 的张量 B 为例，这两个张量可以进行批量矩阵乘法，结果的形状将会是 `[batch_size, n_rows, p]`。\n",
    "\n",
    "例如，如果我们有一个三维张量 A，形状为 `[2, 3, 4]`，和另一个三维张量 B，形状为 `[2, 4, 5]`，这表示我们有两批矩阵，A 的每个矩阵有 3 行 4 列，B 的每个矩阵有 4 行 5 列。两个张量在最内侧维度上的大小（A 的列和 B 的行）是相同的（都是 4），因此它们可以相乘。乘法的结果将会是一个形状为 `[2, 3, 5]` 的三维张量，其中每个 `[3, 5]` 的矩阵是 A 中的一个 `[3, 4]` 矩阵与 B 中的一个 `[4, 5]` 矩阵的乘积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 29.,  67.],\n",
      "        [127., 173.]])\n"
     ]
    }
   ],
   "source": [
    "# 批量矩阵乘向量\n",
    "'''\n",
    "batch_vector.unsqueeze(-1) 是将 batch_vector 张量增加一个维度。\n",
    "在PyTorch中，unsqueeze(-1) 会在张量的最后一个维度后面添加一个维度。\n",
    "这样做通常是为了将一个向量转换为一个列向量，使得它可以与矩阵相乘。\n",
    "如果 batch_vector 的原始形状是 [n, m]，那么 unsqueeze 操作后的形状将会是 [n, m, 1]。\n",
    "\n",
    ".squeeze(-1) 是将乘法结果的最后一个维度（如果它的大小是1）移除。\n",
    "这通常是用来去除单维度条目，将 [n, m, 1] 形状的张量转换为 [n, m]。\n",
    "这样最终的 batch_result 就会是一个二维张量，每行代表对应矩阵-向量乘积的结果。\n",
    "\n",
    "'''\n",
    "import torch\n",
    "\n",
    "# 定义批量矩阵和向量\n",
    "batch_matrix = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=torch.float32)\n",
    "batch_vector = torch.tensor([[9, 10], [11, 12]], dtype=torch.float32)\n",
    "\n",
    "# 批量矩阵乘向量\n",
    "batch_result = torch.matmul(batch_matrix, batch_vector.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "print(batch_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7., 10.],\n",
      "        [15., 22.]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘矩阵（2D张量）\n",
    "import torch\n",
    "\n",
    "matrix_a = torch.tensor([[1,2],[3,4]],dtype = torch.float32)\n",
    "matrix_b = torch.tensor([[1,2],[3,4]],dtype = torch.float32)\n",
    "\n",
    "matrix_product = torch.mm(matrix_a,matrix_b)\n",
    "print(matrix_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3单位矩阵和逆矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 单位矩阵\n",
    "import tensorflow as tf\n",
    "matrix=tf.eye(5)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-2.0000002   1.0000001 ]\n",
      " [ 1.5000001  -0.50000006]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 逆矩阵\n",
    "import tensorflow as tf\n",
    "matrix = tf.constant([[1,2],[3,4]],dtype = tf.float32)\n",
    "inverse_matrix =tf.linalg.inv(matrix)\n",
    "print(inverse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建单位矩阵\n",
    "import torch\n",
    "identity_matrix = torch.eye(3)\n",
    "\n",
    "print(identity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0000,  1.0000],\n",
      "        [ 1.5000, -0.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个矩阵\n",
    "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# 计算逆矩阵\n",
    "inverse_matrix = torch.inverse(matrix)\n",
    "\n",
    "print(inverse_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4线性空间和生成子空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "tf.Tensor(5.477226, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "Frobenius范数: 9.539392\n",
      "无穷范数: 6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "vector = tf.constant([1,2,3,4],dtype = tf.float32)\n",
    "\n",
    "#L1\n",
    "L1_norm = tf.norm(vector,ord=1)\n",
    "print(L1_norm)\n",
    "#L2\n",
    "L1_norm = tf.norm(vector,ord=2)\n",
    "print(L1_norm)\n",
    "#L∞\n",
    "inf_norm = tf.norm(vector,ord=np.inf)\n",
    "print(inf_norm)\n",
    "\n",
    "# 创建一个矩阵\n",
    "matrix = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
    "\n",
    "# 计算Frobenius范数，tf.norm(matrix)默认计算的是矩阵的Frobenius范数，\n",
    "# Frobenius范数可以看作是矩阵元素的欧几里得范数，它是将矩阵的所有元素视为向量的元素并计算其长度。默认即eculidean\n",
    "frobenius_norm = tf.norm(matrix,ord='euclidean')\n",
    "print(\"Frobenius范数:\", frobenius_norm.numpy())\n",
    "\n",
    "# 计算无穷范数\n",
    "inf_norm = tf.norm(matrix, ord=np.inf)\n",
    "print(\"无穷范数:\", inf_norm.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2范数: 3.7416574954986572\n",
      "l2_norm的数据类型: <class 'torch.Tensor'>\n",
      "l2_norm的元素数据类型: torch.float32\n",
      "l2_norm.item()的数据类型: <class 'float'>\n",
      "L1范数: 6.0\n",
      "Frobenius范数: 9.539392471313477\n",
      "无穷范数: 6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vector = torch.tensor([1,2,3],dtype=torch.float32)\n",
    "\n",
    "# 计算L2范数\n",
    "l2_norm = torch.norm(vector, p=2)\n",
    "# 使用.item()将单元素张量转换为标量\n",
    "print(\"L2范数:\", l2_norm.item())\n",
    "\n",
    "# 输出l2_norm的数据类型\n",
    "print(\"l2_norm的数据类型:\", type(l2_norm))\n",
    "print(\"l2_norm的元素数据类型:\", l2_norm.dtype)\n",
    "\n",
    "# 输出l2_norm.item()的数据类型\n",
    "'''\n",
    "l2_norm.item()返回的是一个标准的Python数值（比如整数或浮点数），而不是一个PyTorch张量。\n",
    "因此，它没有一个特定的“元素数据类型”属性，因为它不是一个张量。\n",
    "它的数据类型将是一个标准的Python数据类型，如int或float，这取决于张量中元素的数据类型。\n",
    "'''\n",
    "print(\"l2_norm.item()的数据类型:\", type(l2_norm.item()))\n",
    "\n",
    "# 计算L1范数\n",
    "l1_norm = torch.norm(vector, p=1)\n",
    "print(\"L1范数:\", l1_norm.item())\n",
    "\n",
    "# 创建一个矩阵\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "\n",
    "# 计算Frobenius范数\n",
    "frobenius_norm = torch.norm(matrix, p='fro')\n",
    "print(\"Frobenius范数:\", frobenius_norm.item())\n",
    "\n",
    "# 计算无穷范数\n",
    "inf_norm = torch.norm(matrix, p=float('inf'))\n",
    "print(\"无穷范数:\", inf_norm.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
